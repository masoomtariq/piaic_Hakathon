**Hackathon** **I:** **Create** **a** **Textbook** **for** **Teaching**
**Physical** **AI** **&** **Humanoid** **Robotics** **Course**

The future of work will be a [<u>partnership between people, intelligent
agents (AI software),
and</u>](https://www.mckinsey.com/mgi/our-research/agents-robots-and-us-skill-partnerships-in-the-age-of-ai)
<u>[robots](https://www.mckinsey.com/mgi/our-research/agents-robots-and-us-skill-partnerships-in-the-age-of-ai).</u>
This shift won't necessarily eliminate jobs but will change what humans
do, leading to a massive demand for new skills. We have already written
a book on AI agents. Therefore, we want you to write a textbook to teach
a course in Physical AI & Humanoid Robotics (The course details are
documented below).

**Excel** **in** **the** **Hackathon** **and** **Launch** **Your**
**Journey** **as** **an** **AI** **Startup** **Founder** ðŸš€

Weâ€™ve recently launched **Panaversity** **(panaversity.org)**, an
initiative focused on teaching cutting-edge AI courses. Alongside this,
weâ€™re working on publishing our first book, which you can explore at
**ai-native.panaversity.org**. Our next milestone is to build a portal
where authors can create AI-native technical textbooks, and readers can
easily access and learn from them using AI Agents. We also plan to
publish **O/A** **Level,** **Science,** **Engineering,** **and**
**Medical** AI-native books to support students and professionals across
disciplines. If you perform well in this hackathon, you may be invited
for an interview to join the **Panaversity** **core** **team** and
potentially step into the role of a **startup** **founder** within this
growing ecosystem. You will get a chance to work with Panaversity
founders Zia, Rehan, Junaid, and Wania and become the very best. You may
also get a chance to teach at Panaversity, PIAIC, and GIAIC.

**Requirements**

You are required to complete a unified book project using Claude Code
and Spec-Kit Plus. The core deliverables are:

1\. AI/Spec-Driven Book Creation: Write a book using Docusaurus and
deploy it to GitHub Pages. You will use Spec-Kit Plus (
[<u>https://github.com/panaversity/spec-kit-plus/</u>](https://github.com/panaversity/spec-kit-plus/)
) and Claude Code (
[<u>https://www.claude.com/product/claude-code</u>](https://www.claude.com/product/claude-code)
) to write the book.

2\. Integrated RAG Chatbot Development: Build and embed a
Retrieval-Augmented Generation (RAG) chatbot within the published book.
This chatbot, utilizing the OpenAI Agents/ChatKit SDKs, FastAPI, Neon
Serverless Postgres database, and Qdrant Cloud Free Tier, must be able
to answer user questions about the book's content, including answering
questions based only on text selected by the user.

3\. Participants will receive points out of 100, for base functionality
defined above.

4\. Participants can earn up to 50 extra bonus points by creating and
using reusable intelligence via Claude Code Subagents and Agent Skills
in the book project.

> 1

5\. Participants can receive up to 50 extra bonus points if they also
implement Signup and Signin using
[<u>https://www.better-auth.com/</u>](https://www.better-auth.com/) At
signup you will ask questions from the user about their software and
hardware background. Knowing the background of the user we will be able
to personalize the content.

6\. Participants can receive up to 50 extra bonus points if the logged
user can personalise the content in the chapters by pressing a button at
the start of each chapter.

7\. Participants can receive up to 50 extra bonus points if the logged
user can translate the content in Urdu in the chapters by pressing a
button at the start of each chapter.

**The** **Course** **Details**

**Physical** **AI** **&** **Humanoid** **Robotics**

*Focus* *and* *Theme:* *AI* *Systems* *in* *the* *Physical* *World.*
*Embodied* *Intelligence.* *Goal:* *Bridging* *the* *gap* *between*
*the* *digital* *brain* *and* *the* *physical* *body.* *Students*
*apply* *their* *AI* *knowledge* *to* *control* *Humanoid* *Robots* *in*
*simulated* *and* *real-world* *environments.*

**Quarter** **Overview**

The future of AI extends beyond digital spaces into the physical world.
This capstone quarter introduces Physical AIâ€”AI systems that function in
reality and comprehend physical laws. Students learn to design,
simulate, and deploy humanoid robots capable of natural human
interactions using ROS 2, Gazebo, and NVIDIA Isaac.

> **â—** **Module** **1:** **The** **Robotic** **Nervous** **System**
> **(ROS** **2)** â—‹ Focus: Middleware for robot control.
>
> â—‹ ROS 2 Nodes, Topics, and Services.
>
> â—‹ Bridging Python Agents to ROS controllers using rclpy.
>
> â—‹ Understanding URDF (Unified Robot Description Format) for humanoids.
>
> **â—** **Module** **2:** **The** **Digital** **Twin** **(Gazebo** **&**
> **Unity)**
>
> â—‹ Focus: Physics simulation and environment building. â—‹ Simulating
> physics, gravity, and collisions in Gazebo.
>
> â—‹ High-fidelity rendering and human-robot interaction in Unity. â—‹
> Simulating sensors: LiDAR, Depth Cameras, and IMUs.
>
> **â—** **Module** **3:** **The** **AI-Robot** **Brain** **(NVIDIA**
> **Isaacâ„¢)** â—‹ Focus: Advanced perception and training.
>
> â—‹ NVIDIA Isaac Sim: Photorealistic simulation and synthetic data
> generation.
>
> â—‹ Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation.
>
> 2
>
> â—‹ Nav2: Path planning for bipedal humanoid movement.
>
> **â—** **Module** **4:** **Vision-Language-Action** **(VLA)**
>
> â—‹ Focus: The convergence of LLMs and Robotics.
>
> â—‹ Voice-to-Action: Using OpenAI Whisper for voice commands.
>
> â—‹ Cognitive Planning: Using LLMs to translate natural language ("Clean
> the room") into a sequence of ROS 2 actions.
>
> â—‹ Capstone Project: The Autonomous Humanoid. A final project where a
> simulated robot receives a voice command, plans a path, navigates
> obstacles, identifies an object using computer vision, and manipulates
> it.

**Why** **Physical** **AI** **Matters**

Humanoid robots are poised to excel in our human-centered world because
they share our physical form and can be trained with abundant data from
interacting in human environments. This represents a significant
transition from AI models confined to digital environments to embodied
intelligence that operates in physical space.

**Learning** **Outcomes**

> 1\. Understand Physical AI principles and embodied intelligence 2.
> Master ROS 2 (Robot Operating System) for robotic control 3. Simulate
> robots with Gazebo and Unity
>
> 4\. Develop with NVIDIA Isaac AI robot platform
>
> 5\. Design humanoid robots for natural interactions 6. Integrate GPT
> models for conversational robotics

**Weekly** **Breakdown**

**Weeks** **1-2:** **Introduction** **to** **Physical** **AI**

> â€¢ Foundations of Physical AI and embodied intelligence â€¢ From digital
> AI to robots that understand physical laws â€¢ Overview of humanoid
> robotics landscape
>
> â€¢ Sensor systems: LIDAR, cameras, IMUs, force/torque sensors

**Weeks** **3-5:** **ROS** **2** **Fundamentals**

> â€¢ ROS 2 architecture and core concepts â€¢ Nodes, topics, services, and
> actions
>
> â€¢ Building ROS 2 packages with Python
>
> â€¢ Launch files and parameter management

**Weeks** **6-7:** **Robot** **Simulation** **with** **Gazebo**

> â€¢ Gazebo simulation environment setup
>
> â€¢ URDF and SDF robot description formats â€¢ Physics simulation and
> sensor simulation â€¢ Introduction to Unity for robot visualization
>
> 3

**Weeks** **8-10:** **NVIDIA** **Isaac** **Platform**

> â€¢ NVIDIA Isaac SDK and Isaac Sim
>
> â€¢ AI-powered perception and manipulation â€¢ Reinforcement learning for
> robot control â€¢ Sim-to-real transfer techniques

**Weeks** **11-12:** **Humanoid** **Robot** **Development**

> â€¢ Humanoid robot kinematics and dynamics â€¢ Bipedal locomotion and
> balance control
>
> â€¢ Manipulation and grasping with humanoid hands â€¢ Natural human-robot
> interaction design

**Week** **13:** **Conversational** **Robotics**

> â€¢ Integrating GPT models for conversational AI in robots
>
> â€¢ Speech recognition and natural language understanding â€¢ Multi-modal
> interaction: speech, gesture, vision

**Assessments**

> â€¢ ROS 2 package development project â€¢ Gazebo simulation implementation
>
> â€¢ Isaac-based perception pipeline
>
> â€¢ Capstone: Simulated humanoid robot with conversational AI

**Hardware** **Requirements**

This course is technically demanding. It sits at the intersection of
three heavy computational loads: **Physics** **Simulation** (Isaac
Sim/Gazebo), **Visual** **Perception** (SLAM/Computer Vision), and
**Generative** **AI** (LLMs/VLA).

Because the capstone involves a "Simulated Humanoid," the primary
investment must be in **High-Performance** **Workstations**. However, to
fulfill the "Physical AI" promise, you also need **Edge** **Computing**
**Kits** (brains without bodies) or specific robot hardware.

**1.** **The** **"Digital** **Twin"** **Workstation** **(Required**
**per** **Student)**

This is the most critical component. NVIDIA Isaac Sim is an Omniverse
application that requires "RTX" (Ray Tracing) capabilities. Standard
laptops (MacBooks or non-RTX Windows machines) **will** **not**
**work**.

> â— **GPU** **(The** **Bottleneck):** NVIDIA **RTX** **4070** **Ti**
> **(12GB** **VRAM)** or higher. â—‹ *Why:* You need high VRAM to load the
> USD (Universal Scene
>
> Description) assets for the robot and environment, plus run the VLA
> (Vision-Language-Action) models simultaneously.
>
> 4
>
> â—‹ *Ideal:* RTX 3090 or 4090 (24GB VRAM) allows for smoother
> "Sim-to-Real" training.
>
> â— **CPU:** Intel Core i7 (13th Gen+) or AMD Ryzen 9.
>
> â—‹ *Why:* Physics calculations (Rigid Body Dynamics) in Gazebo/Isaac
> are CPU-intensive.
>
> â— **RAM:** **64** **GB** **DDR5** (32 GB is the absolute minimum, but
> will crash during complex scene rendering).
>
> â— **OS:** **Ubuntu** **22.04** **LTS**.
>
> â—‹ *Note:* While Isaac Sim runs on Windows, ROS 2 (Humble/Iron) is
> native to Linux. Dual-booting or dedicated Linux machines are
> mandatory for a friction-free experience.

**2.** **The** **"Physical** **AI"** **Edge** **Kit**

Since a full humanoid robot is expensive, students learn "Physical AI"
by setting up the *nervous* *system* on a desk before deploying it to a
robot. This kit covers Module 3 (Isaac ROS) and Module 4 (VLA).

> â— **The** **Brain:** **NVIDIA** **Jetson** **Orin** **Nano** (8GB) or
> **Orin** **NX** (16GB).
>
> â—‹ *Role:* This is the industry standard for embodied AI. Students will
> deploy their ROS 2 nodes here to understand resource constraints vs.
> their powerful workstations.
>
> â— **The** **Eyes** **(Vision):** **Intel** **RealSense** **D435i** or
> **D455**.
>
> â—‹ *Role:* Provides RGB (Color) and Depth (Distance) data. Essential
> for the VSLAM and Perception modules.
>
> â— **The** **Inner** **Ear** **(Balance):** Generic USB IMU (BNO055)
> (Often built into the RealSense D435i or Jetson boards, but a separate
> module helps teach IMU calibration).
>
> â— **Voice** **Interface:** A simple USB Microphone/Speaker array
> (e.g., ReSpeaker) for the "Voice-to-Action" Whisper integration.

**3.** **The** **Robot** **Lab**

For the "Physical" part of the course, you have three tiers of options
depending on budget.

**Option** **A:** **The** **"Proxy"** **Approach** **(Recommended**
**for** **Budget)**

Use a quadruped (dog) or a robotic arm as a proxy. The software
principles (ROS 2, VSLAM, Isaac Sim) transfer 90% effectively to
humanoids.

> â— **Robot:** **Unitree** **Go2** **Edu** (~\$1,800 - \$3,000).
>
> â— **Pros:** Highly durable, excellent ROS 2 support, affordable enough
> to have multiple units.
>
> â— **Cons:** Not a biped (humanoid).

**Option** **B:** **The** **"Miniature** **Humanoid"** **Approach**

> 5

Small, table-top humanoids.

> â— **Robot:** **Unitree** **H1** is too expensive (\$90k+), so look at
> **Unitree** **G1** (~\$16k) or **Robotis** **OP3** (older, but stable,
> ~\$12k).
>
> â— **Budget** **Alternative:** **Hiwonder** **TonyPi** **Pro**
> (~\$600).
>
> â—‹ *Warning:* The cheap kits (Hiwonder) usually run on Raspberry Pi,
> which **cannot** run NVIDIA Isaac ROS efficiently. You would use these
> only for kinematics (walking) and use the Jetson kits for AI.

**Option** **C:** **The** **"Premium"** **Lab** **(Sim-to-Real**
**specific)**

If the goal is to actually deploy the Capstone to a real humanoid:

> â— **Robot:** **Unitree** **G1** **Humanoid**.
>
> â—‹ *Why:* It is one of the few commercially available humanoids that
> can actually walk dynamically and has an SDK open enough for students
> to inject their own ROS 2 controllers.

**4.** **Summary** **of** **Architecture**

To teach this successfully, your lab infrastructure should look like
this:

||
||
||
||
||
||
||

If you do not have access to RTX-enabled workstations, we must
restructure the course to rely entirely on cloud-based instances (like
AWS RoboMaker or NVIDIA's

> 6

cloud delivery for Omniverse), though this introduces significant
latency and cost complexity.

Building a "Physical AI" lab is a significant investment. You will have
to choose between building a physical **On-Premise** **Lab** **at**
**Home** (High CapEx) versus running a **Cloud-Native** **Lab** (High
OpEx).

**Option** **2** **High** **OpEx:** **The** **"Ether"** **Lab**
**(Cloud-Native)**

*Best* *for:* *Rapid* *deployment,* *or* *students* *with* *weak*
*laptops.*

**1.** **Cloud** **Workstations** **(AWS/Azure)** Instead of buying PCs,
you rent instances.

> â— **Instance** **Type:** AWS **g5.2xlarge** (A10G GPU, 24GB VRAM) or
> **g6e.xlarge**. â— **Software:** NVIDIA Isaac Sim on Omniverse Cloud
> (requires specific AMI). â— **Cost** **Calculation:**
>
> â—‹ Instance cost: ~\$1.50/hour (spot/on-demand mix). â—‹ Usage: 10
> hours/week Ã— 12 weeks = 120 hours.
>
> â—‹ Storage (EBS volumes for saving environments): ~\$25/quarter. â—‹
> **Total** **Cloud** **Bill:** **~\$205** **per** **quarter**.

**2.** **Local** **"Bridge"** **Hardware** You cannot eliminate hardware
entirely for "Physical AI." You still need the edge devices to deploy
the code physically.

> â— **Edge** **AI** **Kits:** You still need the Jetson Kit for the
> physical deployment phase. â—‹ **Cost:** **\$700** (One-time purchase).
>
> â— **Robot:** You still need one physical robot for the final demo. â—‹
> **Cost:** **\$3,000** (Unitree Go2 Standard).

**The** **Economy** **Jetson** **Student** **Kit**

*Best* *for:* *Learning* *ROS* *2,* *Basic* *Computer* *Vision,* *and*
*Sim-to-Real* *control.*

||
||
||
||
||

> 7

||
||
||
||
||
||

**3.** **The** **Latency** **Trap** **(Hidden** **Cost)**

> â— Simulating in the cloud works well, but *controlling* a real robot
> from a cloud instance is dangerous due to latency.
>
> â— *Solution:* Students train in the Cloud, download the model
> (weights), and flash it to the local Jetson kit.
>
> 8
